# -*- coding: utf-8 -*-
"""Embedding_indexing_QA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-gl5lv0t3AtCNOJS5V6U2f5EROThnjnB
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install langchain-community
# %pip install -qU langchain-together
# %pip install -qU langchain-openai

import numpy as np
import pandas as pd
from langchain.docstore.document import Document
from langchain.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
import tensorflow as tf
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_together import TogetherEmbeddings
from langchain_openai import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain_together import ChatTogether
from langchain.prompts import PromptTemplate

# df = pd.read_csv("output_stream.csv")
# df = df.drop(['time','diff'], axis=1)
# documents = [Document(page_content=str(row['combined_text']).replace("\\n","\n")) for index, row in df.iterrows()]
# # df.head()
# with open('documents2.txt', 'w') as f:
#     for doc in documents:
#         f.write(doc.page_content)

import getpass
import os

if not os.getenv("TOGETHER_API_KEY"):
    os.environ["TOGETHER_API_KEY"] = getpass.getpass("Enter your Together API key: ")

# try:
#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # Detect TPU
#     tf.config.experimental_connect_to_cluster(tpu)
#     tf.tpu.experimental.initialize_tpu_system(tpu)
#     strategy = tf.distribute.TPUStrategy(tpu)
#     print('Running on TPU')
# except ValueError as e:
#     print('Error:', e)

# with strategy.scope():
embeddings = TogetherEmbeddings(
model="togethercomputer/m2-bert-80M-8k-retrieval",
# device="cuda"
)

# embeddings = OpenAIEmbeddings(
#     model="text-embedding-3-large",
# )
loader = TextLoader('documents_small.txt')
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size=250,
    chunk_overlap=0,
    # length_function=len,
    # is_separator_regex=False,
)

docs = text_splitter.split_documents(documents)
texts = [doc.page_content for doc in docs]
# text2 = "sahil is a good person with a briliant brain also with evry good looks"
vectorstore = InMemoryVectorStore.from_texts(
    texts,
    embedding=embeddings,
    # device='cuda'
)

# Use the vectorstore as a retriever
retriever = vectorstore.as_retriever()

question = "Show me information about the city with zip code 30129."
docs_ss = vectorstore.similarity_search(question,k=3)
for doc in docs_ss:
    print(doc.page_content)

LLM = ChatTogether(
    model="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
)

template = """You are a highly knowledgeable assistant with access to a comprehensive geographical database.
The database contains detailed information about
zip codes, type, primary_city, acceptable_cities, unacceptable_cities,
states, county, timezone, country,
of various locations.

Given the following query and the relevant information from the database:

{context}
Question: {question}

Extract and provide the relevant details from the context, such as the city, state, county, timezone,
and any other relevant information. If specific details are unavailable in the context, respond accordingly.

Please provide a detailed response using the available data.
Helpful Answer:"""

QA_CHAIN_PROMPT = PromptTemplate.from_template(template)

qa_chain = RetrievalQA.from_chain_type(
    LLM,
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
)

question2 = "Show me information about the city with zip code 30129."
# Pass the query as a dictionary with the 'query' key
result = qa_chain({"query": question2})
print(result["result"])